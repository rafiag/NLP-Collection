{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of NLP",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8YNXquv5Bgx",
        "colab_type": "text"
      },
      "source": [
        "####Importing required libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tnq5sCQ-xPi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for error handling\n",
        "import sys\n",
        "\n",
        "# for tokenizeWords\n",
        "import nltk\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "nltk.download('punkt')\n",
        "\n",
        "# for tokenizeStemmedWords\n",
        "!pip install Sastrawi\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "\n",
        "# for tokenizeEmojis\n",
        "!pip install emoji\n",
        "import emoji\n",
        "import re\n",
        "\n",
        "# for tokenizeUrls\n",
        "import http.client as httplib\n",
        "import urllib.parse as urlparse\n",
        "import requests\n",
        "\n",
        "import math\n",
        "import decimal"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OstTRPgI5KiQ",
        "colab_type": "text"
      },
      "source": [
        "####Defining function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvZBXeWm_RAD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenizeWords(s, remove_punctuation=True):\n",
        "  \"\"\"\n",
        "  Return array[string] of each word of the text.\n",
        "  Params:\n",
        "  s: string to be tokenized\n",
        "  remove_punctuation: remove punctuation using RegexpTokenizer (default to False)\n",
        "  \"\"\"\n",
        "  if remove_punctuation == True:\n",
        "    tokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')\n",
        "    clean_words = tokenizer.tokenize(s)\n",
        "  else:\n",
        "    clean_words = nltk.word_tokenize(s)\n",
        "  return clean_words\n",
        "\n",
        "def tokenizeStemmedWords(s):\n",
        "  \"\"\"\n",
        "  Return array[string] of the text with each word has been stemmed to it's root form.\n",
        "  Params:\n",
        "  s: text to be processed\n",
        "  \"\"\"\n",
        "  clean_stemmed_words = tokenizeWords(s)\n",
        "  factory = StemmerFactory()\n",
        "  stemmer = factory.create_stemmer()\n",
        "  for n, word in enumerate(clean_stemmed_words):\n",
        "    clean_stemmed_words[n] = stemmer.stem(word)\n",
        "    if clean_stemmed_words[n] == '':\n",
        "      clean_stemmed_words.pop(n)\n",
        "  return clean_stemmed_words\n",
        "\n",
        "def tokenizeEmojis(s):\n",
        "  \"\"\"\n",
        "  Return array[string] of emoji's alias that exist in the text.\n",
        "  Params:\n",
        "  s: text to extract emoji's alias from\n",
        "  \"\"\"\n",
        "  emojis_alias = []\n",
        "  words = re.findall(r'[^\\w\\s,]', s)\n",
        "  for word in words:\n",
        "    if any(char in emoji.UNICODE_EMOJI for char in word):\n",
        "      emojis_alias.append(word)\n",
        "  for n, char in enumerate(emojis_alias):\n",
        "    emojis_alias[n] = emoji.demojize(char)\n",
        "  return emojis_alias\n",
        "\n",
        "def tokenizeUrls(s):\n",
        "  \"\"\"\n",
        "  Return array[string] of all expanded urls from all shortened urls that exis in the text.\n",
        "  Params:\n",
        "  s: text to extract expanded url from\n",
        "  \"\"\"\n",
        "  urls = re.findall(r'(https?://\\S+)', s)\n",
        "  for n, url in enumerate(urls):\n",
        "    m = re.search(r',$', url)\n",
        "    if m != None:\n",
        "      url = url[:-1]\n",
        "    try:\n",
        "      urls[n] = requests.head(url, allow_redirects=True).url\n",
        "    except Exception as ex:\n",
        "      print('{0} when handling {1}' .format(sys.exc_info()[0], urls[n]))\n",
        "  return urls\n",
        "\n",
        "def tokenizeHashtags(s):\n",
        "  \"\"\"\n",
        "  Return array[string] of all hashtags that exis in the text.\n",
        "  Params:\n",
        "  s: text to extract hashtags from\n",
        "  \"\"\"\n",
        "  hashtags = [word for word in s.split() if word.startswith(\"#\")]\n",
        "  for n, tag in enumerate(hashtags):\n",
        "    hashtags[n] = re.sub(r\"[!?.,]\", \"\", tag)\n",
        "  return hashtags\n",
        "\n",
        "def numToWords(num):\n",
        "  \"\"\"\n",
        "  Convert number into their Indonesian words.\n",
        "  Params:\n",
        "  num: numnber to convert\n",
        "  \"\"\"\n",
        "  d = {0: 'nol', 1: 'satu', 2: 'dua', 3: 'tiga', 4: 'empat', 5: 'lima',\n",
        "       6: 'enam', 7: 'tujuh', 8: 'delapan', 9: 'sembilan', 10: 'sepuluh',\n",
        "       11: 'sebelas', 12: 'dua belas', 13: 'tiga belas', 14: 'empat belas',\n",
        "       15: 'lima belas', 16: 'enam belas', 17: 'tujuh belas', 18: 'delapan belas',\n",
        "       19: 'sembilan belas', 20: 'dua puluh', 30: 'tiga puluh', 40: 'empat puluh',\n",
        "       50: 'lima puluh', 60: 'enam puluh', 70: 'tujuh puluh', 80: 'delapan puluh',\n",
        "       90: 'sembilan puluh'}\n",
        "  ribu = 1000\n",
        "  juta = ribu * 1000\n",
        "  miliar = juta * 1000\n",
        "  triliun = miliar * 1000\n",
        "\n",
        "  if type(num) == str:\n",
        "    if '.' in num:\n",
        "      num = float(num)\n",
        "    else: num = int(num)\n",
        "    \n",
        "  if (num < 0):\n",
        "    return 'minus ' + numToWords(num * -1)\n",
        "\n",
        "  if type(num) == float:\n",
        "    float_int, float_dec = FloatToInt(num)\n",
        "    normalized_numbers = numToWords(float_int) + ' koma'\n",
        "    for words_num in [numToWords(d) for d in str(float_dec)]:\n",
        "      normalized_numbers += ' ' + words_num\n",
        "    return normalized_numbers\n",
        "\n",
        "  if (num < 20):\n",
        "    return d[num]\n",
        "\n",
        "  if (num < 100):\n",
        "    if num % 10 == 0: return d[num]\n",
        "    else: return d[num // 10 * 10] + ' ' + d[num % 10]\n",
        "\n",
        "  if (num < ribu):\n",
        "    if num // 100 == 1:\n",
        "      if num % 100 == 0: return 'seratus'\n",
        "      else: return 'seratus ' + numToWords(num % 100)\n",
        "    elif num % 100 == 0: return d[num // 100] + ' ratus'\n",
        "    else: return d[num // 100] + ' ratus ' + numToWords(num % 100)\n",
        "\n",
        "  if (num < juta):\n",
        "    if num // ribu == 1:\n",
        "      if num % ribu == 0: return 'seribu'\n",
        "      else: return 'seribu ' + numToWords(num % ribu)\n",
        "    elif num % ribu == 0: return numToWords(num // ribu) + ' ribu'\n",
        "    else: return numToWords(num // ribu) + ' ribu ' + numToWords(num % ribu)\n",
        "\n",
        "  if (num < miliar):\n",
        "    if (num % juta) == 0: return numToWords(num // juta) + ' juta'\n",
        "    else: return numToWords(num // juta) + ' juta ' + numToWords(num % juta)\n",
        "\n",
        "  if (num < triliun):\n",
        "    if (num % miliar) == 0: return numToWords(num // miliar) + ' miliar'\n",
        "    else: return numToWords(num // miliar) + ' miliar ' + numToWords(num % miliar)\n",
        "\n",
        "  if (num % triliun == 0): return numToWords(num // triliun) + ' triliun'\n",
        "  else: return numToWords(num // triliun) + ' triliun ' + numToWords(num % triliun)\n",
        "\n",
        "  raise AssertionError('num is too large: %s' % str(num))\n",
        "\n",
        "def normalizeNumbers(s):\n",
        "  \"\"\"\n",
        "  Return string of text with all the number have been converted into words.\n",
        "  Params:\n",
        "  s: text to extract number from and convert it to words.\n",
        "  \"\"\"\n",
        "  normalized_numbers_in_text = tokenizeWords(s, False)\n",
        "  find_number = re.compile(\"[-+]?\\d*?.\\d+|\\d+\")\n",
        "  for n, word in enumerate(normalized_numbers_in_text):\n",
        "    num = normalized_numbers_in_text[n]\n",
        "    num1 = find_number.search(num)\n",
        "    if num1 != None:\n",
        "      normalized_numbers_in_text[n] = numToWords(num1.group())\n",
        "  return normalized_numbers_in_text\n",
        "\n",
        "def FloatToInt(float_num):\n",
        "  \"\"\"\n",
        "  Split a float number into two integer and decimal part.\n",
        "  Params:\n",
        "  float_num: float number\n",
        "  float_int: integer part of the input float number\n",
        "  float_dec: decimal part of the input float number.\n",
        "  \"\"\"\n",
        "  if type(float_num) != float:\n",
        "    float_num = float(float_num)\n",
        "\n",
        "  float_dec, float_int = math.modf(float_num)\n",
        "\n",
        "  float_int = int(float_int)\n",
        "  float_dec = round(float_dec - int(float_dec), 10)\n",
        "  d = decimal.Decimal(str(float_dec))\n",
        "  d = d.as_tuple().exponent * -1\n",
        "  float_dec = int(float_dec * 10**d)\n",
        "  return float_int, float_dec"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "709FJVky47Ma",
        "colab_type": "text"
      },
      "source": [
        "####Data for testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Py3WBojL-9Jc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "s = \"\"\"Bagi saya, cerita ini dimulai dari 15 tahun yang lalu.\n",
        "ketika saya adalah seorang dokter di Universitas Chicago.\n",
        "Dan saya sedang merawat orang-orang yang sedang sekarat dan keluarganya.\n",
        "di bagian selatan di Chicago.\n",
        "Dan saya memperhatikan apa yang terjadi dengan orang-orang dan keluarga mereka.\n",
        "ketika menerima perawatan untuk penyakit kronis mereka.\n",
        "Dan di laboratorium saya, saya mempelajari widower effect,\n",
        "yang merupakan suatu ide lama dalam ilmu sosial,\n",
        "dari 150 tahun yang lalu.\n",
        "terkenal dengan sebutan \"meninggal karena patah hati.\"\n",
        "Jadi, ketika saya meninggal, resiko kematian istri saya dapat meningkat dua kali lipat.\n",
        "sebagai contoh, di tahun pertama.\n",
        "Dan saya telah merawat satu orang pasien,\n",
        "seorang wanita yang terkena penyakit dementia.\n",
        "Dan dalam kasus ini, tidak seperti pasangan ini,\n",
        "dia dirawat\n",
        "oleh anak perempuannya.\n",
        "Dan anak perempuannya kelelahan karena merawat ibunya.\n",
        "Dan suami anak perempuan tersebut,\n",
        "dia juga sakit\n",
        "dari kelelahan istrinya.\n",
        "Dan saya sedang menyetir menuju ke rumah,\n",
        "dan saya menerima telepon dari teman suami tersebut,\n",
        "memanggil saya karena dia stress\n",
        "tentang apa yang terjadi dengan temannya.\n",
        "Jadi saya mendapat telepon dari orang asing ini\n",
        "menunjukkan suatu kejadian\n",
        "yang terjadi karena terpengaruh oleh orang lain\n",
        "pada jarak sosial tertentu.\n",
        "Dan saya tiba-tiba menyadari dua hal yang sangat sederhana:\n",
        "Satu, efek widowhood\n",
        "tidak terbatas pada suami dan istri saja\n",
        "Dan kedua, tidak terbatas pada pasangan orang-orang terdekat.\n",
        "Dan saya mulai melihat dunia\n",
        "dengan cara yang baru,\n",
        "seperti orang-orang yang terhubung satu sama lain.\n",
        "Dan saya menyadari bahwa orang-orang ini\n",
        "terhubung oleh empat orang dari pasangan orang-orang terdekat.\n",
        "Dan kemudian, sebenarnya, orang-orang ini\n",
        "terikat dalam hubungan yang lain:\n",
        "pernikahan dan pasangan hidup\n",
        "dan pertemanan dan ikatan yang lainnya\n",
        "Dan sebenarnya, hubungan ini sangatlah luas\n",
        "dan kita semua terikat di sini\n",
        "hubungan yang sangat luas satu sama lain.\"\"\"\n",
        "\n",
        "emoji_test = \"Lorem ipsum ðŸ‘©ðŸ¾â€ðŸŽ“ dolor sit amet ðŸ¤” ðŸ™ˆ, consectetur ðŸ˜ŒðŸ˜Š adipiscing ðŸ’•ðŸ‘­ðŸ‘™ elit. ðŸ‘¨â€ðŸ‘©â€ðŸ‘¦â€ðŸ‘¦ Nunc rutrum. ðŸ™…ðŸ½ðŸ™…ðŸ½\"\n",
        "\n",
        "url_test = \"Here is an url https://id.wikipedia.org/wiki/Koronavirus, here is a direct url https://www.twitter.com/WatchmenID, and here is another shortened url for you http://bit.ly/ModulBDDA2019.\"\n",
        "\n",
        "hashtag_test = \"This is a #sample text to test #hashtag extraction for #NLP.\"\n",
        "\n",
        "num_int_test = 1212849542\n",
        "num_str_test = \"9827412183\"\n",
        "\n",
        "normal_int = \"Normal number: 5131\"\n",
        "minus_int = \"Minus number: -207\"\n",
        "normal_float = \"Float number:  3.14\"\n",
        "minus_float = \"Minus float: -1.5\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWuQGthZ5N_6",
        "colab_type": "text"
      },
      "source": [
        "####Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U__PJVT6JKHh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        },
        "outputId": "8ace65b2-d778-46ae-eeec-9ef5f8f34045"
      },
      "source": [
        "clean_words = tokenizeWords(s)\n",
        "clean_stemmed_words = tokenizeStemmedWords(s)\n",
        "emojis_alias = tokenizeEmojis(emoji_test)\n",
        "hashtags = tokenizeHashtags(hashtag_test)\n",
        "word_number = numToWords(num_str_test)\n",
        "urls = tokenizeUrls(url_test)\n",
        "normalized_num = normalizeNumbers(normal_int)\n",
        "normalized_minus_num = normalizeNumbers(minus_int)\n",
        "normalized_float_num = normalizeNumbers(normal_float)\n",
        "normalized_minus_float_num = normalizeNumbers(minus_float)\n",
        "\n",
        "print('tokenizeWords result:')\n",
        "print('words found: ' + str(len(clean_words)))\n",
        "print(clean_words)\n",
        "print('-----------------------------------------------------------------------')\n",
        "print('tokenizeStemmedWords result:')\n",
        "print('stemmed words: ' + str(len(clean_stemmed_words)))\n",
        "print(clean_stemmed_words)\n",
        "print('-----------------------------------------------------------------------')\n",
        "print('tokenizeEmojis result:')\n",
        "print('emojis found : ' + str(len(emojis_alias)))\n",
        "print(emojis_alias)\n",
        "print('-----------------------------------------------------------------------')\n",
        "print('tokenizeHashtags result:')\n",
        "print('hashtags found: ' + str(len(hashtags)))\n",
        "print(hashtags)\n",
        "print('-----------------------------------------------------------------------')\n",
        "print('numToWords result:')\n",
        "print('input number: ' + str(num_str_test))\n",
        "print(word_number)\n",
        "print('-----------------------------------------------------------------------')\n",
        "print('tokenizeUrls result:')\n",
        "print('urls found: ' + str(len(urls)))\n",
        "print(urls)\n",
        "print('-----------------------------------------------------------------------')\n",
        "print('normalized_num result:', normalized_num)\n",
        "print('normalized_minus_num result:', normalized_minus_num)\n",
        "print('normalized_float_num result:', normalized_float_num)\n",
        "print('normalized_minus_float_num result:', normalized_minus_float_num)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tokenizeWords result:\n",
            "words found: 334\n",
            "['Bagi', 'saya', ',', 'cerita', 'ini', 'dimulai', 'dari', '15', 'tahun', 'yang', 'lalu', '.', 'ketika', 'saya', 'adalah', 'seorang', 'dokter', 'di', 'Universitas', 'Chicago', '.', 'Dan', 'saya', 'sedang', 'merawat', 'orang', '-orang', 'yang', 'sedang', 'sekarat', 'dan', 'keluarganya', '.', 'di', 'bagian', 'selatan', 'di', 'Chicago', '.', 'Dan', 'saya', 'memperhatikan', 'apa', 'yang', 'terjadi', 'dengan', 'orang', '-orang', 'dan', 'keluarga', 'mereka', '.', 'ketika', 'menerima', 'perawatan', 'untuk', 'penyakit', 'kronis', 'mereka', '.', 'Dan', 'di', 'laboratorium', 'saya', ',', 'saya', 'mempelajari', 'widower', 'effect', ',', 'yang', 'merupakan', 'suatu', 'ide', 'lama', 'dalam', 'ilmu', 'sosial', ',', 'dari', '150', 'tahun', 'yang', 'lalu', '.', 'terkenal', 'dengan', 'sebutan', '\"meninggal', 'karena', 'patah', 'hati', '.\"', 'Jadi', ',', 'ketika', 'saya', 'meninggal', ',', 'resiko', 'kematian', 'istri', 'saya', 'dapat', 'meningkat', 'dua', 'kali', 'lipat', '.', 'sebagai', 'contoh', ',', 'di', 'tahun', 'pertama', '.', 'Dan', 'saya', 'telah', 'merawat', 'satu', 'orang', 'pasien', ',', 'seorang', 'wanita', 'yang', 'terkena', 'penyakit', 'dementia', '.', 'Dan', 'dalam', 'kasus', 'ini', ',', 'tidak', 'seperti', 'pasangan', 'ini', ',', 'dia', 'dirawat', 'oleh', 'anak', 'perempuannya', '.', 'Dan', 'anak', 'perempuannya', 'kelelahan', 'karena', 'merawat', 'ibunya', '.', 'Dan', 'suami', 'anak', 'perempuan', 'tersebut', ',', 'dia', 'juga', 'sakit', 'dari', 'kelelahan', 'istrinya', '.', 'Dan', 'saya', 'sedang', 'menyetir', 'menuju', 'ke', 'rumah', ',', 'dan', 'saya', 'menerima', 'telepon', 'dari', 'teman', 'suami', 'tersebut', ',', 'memanggil', 'saya', 'karena', 'dia', 'stress', 'tentang', 'apa', 'yang', 'terjadi', 'dengan', 'temannya', '.', 'Jadi', 'saya', 'mendapat', 'telepon', 'dari', 'orang', 'asing', 'ini', 'menunjukkan', 'suatu', 'kejadian', 'yang', 'terjadi', 'karena', 'terpengaruh', 'oleh', 'orang', 'lain', 'pada', 'jarak', 'sosial', 'tertentu', '.', 'Dan', 'saya', 'tiba', '-tiba', 'menyadari', 'dua', 'hal', 'yang', 'sangat', 'sederhana', ':', 'Satu', ',', 'efek', 'widowhood', 'tidak', 'terbatas', 'pada', 'suami', 'dan', 'istri', 'saja', 'Dan', 'kedua', ',', 'tidak', 'terbatas', 'pada', 'pasangan', 'orang', '-orang', 'terdekat', '.', 'Dan', 'saya', 'mulai', 'melihat', 'dunia', 'dengan', 'cara', 'yang', 'baru', ',', 'seperti', 'orang', '-orang', 'yang', 'terhubung', 'satu', 'sama', 'lain', '.', 'Dan', 'saya', 'menyadari', 'bahwa', 'orang', '-orang', 'ini', 'terhubung', 'oleh', 'empat', 'orang', 'dari', 'pasangan', 'orang', '-orang', 'terdekat', '.', 'Dan', 'kemudian', ',', 'sebenarnya', ',', 'orang', '-orang', 'ini', 'terikat', 'dalam', 'hubungan', 'yang', 'lain', ':', 'pernikahan', 'dan', 'pasangan', 'hidup', 'dan', 'pertemanan', 'dan', 'ikatan', 'yang', 'lainnya', 'Dan', 'sebenarnya', ',', 'hubungan', 'ini', 'sangatlah', 'luas', 'dan', 'kita', 'semua', 'terikat', 'di', 'sini', 'hubungan', 'yang', 'sangat', 'luas', 'satu', 'sama', 'lain', '.']\n",
            "-----------------------------------------------------------------------\n",
            "tokenizeStemmedWords result:\n",
            "stemmed words: 293\n",
            "['bagi', 'saya', 'cerita', 'ini', 'mulai', 'dari', '15', 'tahun', 'yang', 'lalu', 'ketika', 'saya', 'adalah', 'orang', 'dokter', 'di', 'universitas', 'chicago', 'Dan', 'saya', 'sedang', 'rawat', 'orang', '-orang', 'yang', 'sedang', 'sekarat', 'dan', 'keluarga', 'di', 'bagi', 'selatan', 'di', 'chicago', 'Dan', 'saya', 'perhati', 'apa', 'yang', 'jadi', 'dengan', 'orang', '-orang', 'dan', 'keluarga', 'mereka', 'ketika', 'terima', 'awat', 'untuk', 'sakit', 'kronis', 'mereka', 'Dan', 'di', 'laboratorium', 'saya', 'saya', 'ajar', 'widower', 'effect', 'yang', 'rupa', 'suatu', 'ide', 'lama', 'dalam', 'ilmu', 'sosial', 'dari', '150', 'tahun', 'yang', 'lalu', 'terkenal', 'dengan', 'sebut', 'tinggal', 'karena', 'patah', 'hati', 'Jadi', 'ketika', 'saya', 'tinggal', 'resiko', 'mati', 'istri', 'saya', 'dapat', 'tingkat', 'dua', 'kali', 'lipat', 'sebagai', 'contoh', 'di', 'tahun', 'pertama', 'Dan', 'saya', 'telah', 'rawat', 'satu', 'orang', 'pasien', 'seorang', 'wanita', 'yang', 'kena', 'sakit', 'dementia', 'Dan', 'dalam', 'kasus', 'ini', 'tidak', 'seperti', 'pasang', 'ini', 'dia', 'rawat', 'oleh', 'anak', 'perempuan', 'Dan', 'anak', 'perempuan', 'lelah', 'karena', 'rawat', 'ibu', 'Dan', 'suami', 'anak', 'perempuan', 'sebut', 'dia', 'juga', 'sakit', 'dari', 'lelah', 'istri', 'Dan', 'saya', 'sedang', 'setir', 'tuju', 'ke', 'rumah', 'dan', 'saya', 'terima', 'telepon', 'dari', 'teman', 'suami', 'sebut', 'memanggil', 'saya', 'karena', 'dia', 'stress', 'tentang', 'apa', 'yang', 'jadi', 'dengan', 'teman', 'Jadi', 'saya', 'dapat', 'telepon', 'dari', 'orang', 'asing', 'ini', 'tunjuk', 'suatu', 'jadi', 'yang', 'jadi', 'karena', 'pengaruh', 'oleh', 'orang', 'lain', 'pada', 'jarak', 'sosial', 'tentu', 'Dan', 'saya', 'tiba', '-tiba', 'sadar', 'dua', 'hal', 'yang', 'sangat', 'sederhana', 'Satu', 'efek', 'widowhood', 'tidak', 'batas', 'pada', 'suami', 'dan', 'istri', 'saja', 'dan', 'dua', 'tidak', 'batas', 'pada', 'pasang', 'orang', '-orang', 'dekat', 'Dan', 'saya', 'mulai', 'lihat', 'dunia', 'dengan', 'cara', 'yang', 'baru', 'seperti', 'orang', '-orang', 'yang', 'hubung', 'satu', 'sama', 'lain', 'Dan', 'saya', 'sadar', 'bahwa', 'orang', '-orang', 'ini', 'hubung', 'oleh', 'empat', 'orang', 'dari', 'pasang', 'orang', '-orang', 'dekat', 'Dan', 'kemudian', 'sebenarnya', 'orang', '-orang', 'ini', 'ikat', 'dalam', 'hubung', 'yang', 'lain', 'pernikahan', 'dan', 'pasang', 'hidup', 'dan', 'teman', 'dan', 'ikat', 'yang', 'lain', 'dan', 'benar', 'hubungan', 'ini', 'sangat', 'luas', 'dan', 'kita', 'semua', 'ikat', 'di', 'sini', 'hubung', 'yang', 'sangat', 'luas', 'satu', 'sama', 'lain']\n",
            "-----------------------------------------------------------------------\n",
            "tokenizeEmojis result:\n",
            "emojis found : 18\n",
            "[':woman:', ':medium_dark_skin_tone:', ':graduation_cap:', ':thinking_face:', ':see-no-evil_monkey:', ':relieved_face:', ':smiling_face_with_smiling_eyes:', ':two_hearts:', ':two_women_holding_hands:', ':bikini:', ':man:', ':woman:', ':boy:', ':boy:', ':person_gesturing_NO:', ':medium_skin_tone:', ':person_gesturing_NO:', ':medium_skin_tone:']\n",
            "-----------------------------------------------------------------------\n",
            "tokenizeHashtags result:\n",
            "hashtags found: 3\n",
            "['#sample', '#hashtag', '#NLP']\n",
            "-----------------------------------------------------------------------\n",
            "numToWords result:\n",
            "input number: 9827412183\n",
            "sembilan miliar delapan ratus dua puluh tujuh juta empat ratus dua belas ribu seratus delapan puluh tiga\n",
            "-----------------------------------------------------------------------\n",
            "tokenizeUrls result:\n",
            "urls found: 3\n",
            "['https://id.wikipedia.org/wiki/Koronavirus', 'https://twitter.com/WatchmenID', 'https://www.dropbox.com/sh/p5sbl6quxgnpqy4/AAAqHMHvkthWzKmmnM_yu6hXa?dl=0']\n",
            "-----------------------------------------------------------------------\n",
            "normalized_num result: ['Normal', 'number', ':', 'lima ribu seratus tiga puluh satu']\n",
            "normalized_minus_num result: ['Minus', 'number', ':', 'minus dua ratus tujuh']\n",
            "normalized_float_num result: ['Float', 'number', ':', 'tiga koma satu empat']\n",
            "normalized_minus_float_num result: ['Minus', 'float', ':', 'minus satu koma lima']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}