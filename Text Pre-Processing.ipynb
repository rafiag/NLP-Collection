{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Text Pre-Processing.ipynb","provenance":[{"file_id":"1MTpMncnSegAls0vyEb9n2zF-Vsto0DWx","timestamp":1588130242916}],"collapsed_sections":["XHjU5-AJnnZB"],"toc_visible":true,"authorship_tag":"ABX9TyNC4uUyeQ7tgP/MZMjchAaU"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"bclynIUvURM9","colab_type":"text"},"source":["###Setup"]},{"cell_type":"markdown","metadata":{"id":"exTSOgUhUIFi","colab_type":"text"},"source":["####Importing library"]},{"cell_type":"code","metadata":{"id":"lbvwArKhE_6R","colab_type":"code","colab":{}},"source":["# Basic libraries\n","import os\n","import pickle\n","import pandas as pd\n","import numpy as np\n","import warnings\n","warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Text Pre-Processing\n","# Tokenize\n","import re\n","nltk.download('punkt')\n","\n","# Lemmatization\n","import nltk\n","from nltk.stem import WordNetLemmatizer\n","from nltk.corpus import wordnet\n","nltk.download('wordnet')\n","nltk.download('averaged_perceptron_tagger')\n","\n","# Stopword Removal\n","import spacy\n","# from spacy.lang.id.stop_words import STOP_WORDS\n","!python3 -m spacy download en\n","from nltk.corpus import stopwords\n","nltk.download('stopwords')\n","\n","# N-gram\n","from nltk import ngrams\n","from sklearn.feature_extraction.text import CountVectorizer"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"42yOPs1FrG1f","colab_type":"text"},"source":["####Defining Function"]},{"cell_type":"code","metadata":{"id":"GYEFp2iQV9_x","colab_type":"code","colab":{}},"source":["def tokenize(sentence):\n","    sentence = str(sentence)\n","    \n","    # Keep alphanumeric chars\n","    tokens = re.sub(r'[\\W_]+', ' ', sentence)\n","\n","    # Remove numbers\n","    tokens = re.sub(r'\\d+', '', tokens)\n","\n","    # Tokenize\n","    tokens = nltk.word_tokenize(tokens)\n","\n","    # Transform into lowercase and remove word shorter than 3 chararcters\n","    tokens = [word.lower() for word in tokens if len(word)>3]\n","\n","    return tokens\n","\n","def remove_stopwords(sentence):\n","    words = []\n","    for word in sentence:\n","        if str(word) not in stop_words:\n","            words.append(word)\n","    return words\n","    \n","def make_bigrams(texts):\n","    # return [bigram_mod[doc] for doc in texts]\n","    return bigram_mod[texts]\n","\n","def make_trigrams(texts):\n","    return trigram_mod[bigram_mod[texts]]\n","\n","def get_wordnet_pos(word):\n","    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n","    tag = nltk.pos_tag([word])[0][1][0].upper()\n","    tag_dict = {\"J\": wordnet.ADJ,\n","                \"N\": wordnet.NOUN,\n","                \"V\": wordnet.VERB,\n","                \"R\": wordnet.ADV}\n","    return tag_dict.get(tag, wordnet.NOUN)\n","\n","def lemmatization_nltk(word):\n","    lemmatizer = WordNetLemmatizer()\n","    lemmatized = lemmatizer.lemmatize(word, get_wordnet_pos(word))\n","    return lemmatized\n","\n","def lemmatization_spacy(sentence, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n","    \"\"\"https://spacy.io/api/annotation\"\"\"\n","    doc = nlp(\" \".join(sentence))\n","    words = []\n","    for token in doc:\n","        if token.pos_ in allowed_postags: words.append(token.lemma_)\n","    return words\n","\n","nlp = spacy.load('en', disable=['parser', 'ner'])\n","\n","# Build stop words list using NLTK\n","stop_words = stopwords.words('english')\n","stop_words.extend(['from', 'subject', 're', 'edu', 'use', 'after', 'able', 'at'])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"idj10ghoUrkB","colab_type":"text"},"source":["####Loading data"]},{"cell_type":"code","metadata":{"id":"9mETc1ZrPtFL","colab_type":"code","colab":{}},"source":["# Set working folder\n","root = '/content/drive/My Drive/isi_nama_folder_disini'\n","\n","# Load csv file\n","df = pd.read_csv(root+'/nama_file.csv')\n","print('Row: ' + str(df.shape[0]) + '\\n' + 'Column: ' + str(df.shape[1]))\n","df.head()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wzztp9S6U8TL","colab_type":"text"},"source":["\n","###Text Pre-Processing"]},{"cell_type":"markdown","metadata":{"id":"WALNZXF5obGs","colab_type":"text"},"source":["####Tokenize, Stopwords, Lemmatize"]},{"cell_type":"code","metadata":{"id":"w2aKDObd-eHL","colab_type":"code","colab":{}},"source":["col = 'example' # Name of the column where the text data is stored in the csv file\n","\n","# Tokenize\n","df['tokenized'] = df[col].apply(tokenize)\n","\n","# Remove stopword\n","df['nostops'] = df.tokenized.apply(remove_stopwords)\n","\n","# Lemmatize using spacy\n","df['lemmatized'] = df.nostops.apply(lemmatization_spacy, allowed_postags=['NOUN', 'VERB'])\n","\n","# Lemmatize using NLTK\n","# df['lemmatized'] = ''\n","# for idx, row in df.iterrows():\n","#     sent = row['bigrams']\n","#     for n, word in enumerate(row['nostops']):\n","#         sent[n] = lemmatization_nltk(word)\n","#     row['lemmatized'] = sent\n","\n","print('Row: ' + str(df.shape[0]) + '\\n' + 'Column: ' + str(df.shape[1]))\n","df.head()\n","\n","# Save file to Google Drive\n","df.to_csv(root+'/nama_file_baru.csv')"],"execution_count":0,"outputs":[]}]}